# -*- coding: utf-8 -*-
"""Parkinsons 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15K9JyvMRrOZ4wZW8u_UAMitn53rOWRur

#Parkinson Detection dataset using multiple model

The purpose of the dataset exploration is to detect the outcome of parkinson based on the given attribute as follows:

Name - ASCII subject name and recording number
MDVP:Fo(Hz) - Average vocal fundamental frequency
MDVP:Fhi(Hz) - Maximum vocal fundamental frequency
MDVP:Flo(Hz) - Minimum vocal fundamental frequency

MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency

MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude

NHR,HNR - Two measures of ratio of noise to tonal components in the voice

status - Health status of the subject (one) - Parkinson's, (zero) - healthy

RPDE,D2 - Two nonlinear dynamical complexity measures

DFA - Signal fractal scaling exponent

spread1,spread2,PPE - Three nonlinear measures of
fundamental frequency variation
"""

import pandas as pd #Dataframe
import numpy as np #array manipulation
import matplotlib.pyplot as plt #visualization
import seaborn as sns #Visualization

from sklearn.model_selection import train_test_split  #Splitting train and test data

#Valuation Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report

df = pd.read_csv("/content/drive/MyDrive/Python, Data Mining, ETC/Datasets/Parkinsson disease.csv")
df

df.describe

df.info()

df.columns

df = df.drop(columns='name', axis=1)

"""#Dropping name column as it doesn't have correlation"""

df

df.rename(columns=({'MDVP:Fo(Hz)':'average_frequency',
           'MDVP:Fhi(Hz)':'max_frequency',
           'MDVP:Flo(Hz)':'min_frequency',
           'MDVP:Jitter(%)':'frequency_%',
           'MDVP:Jitter(Abs)':'frequency_absolute',
           'MDVP:RAP':'frequency_rap',
           'MDVP:PPQ':'frequency_ppq',
           'MDVP:Shimmer':'amp_var1',
           'MDVP:Shimmer(dB)':'amp_var2',
           'Shimmer:APQ3':'amp_var3',
           'Shimmer:APQ5':'amp_var4',
           'MDVP:APQ':'amp_var5',
           'Shimmer:DDA':'amp_var6',
           }), inplace=True)

df

# Check null value
df.isna().sum() / len(df) * 100

df_corr = df.corr()
plt.figure(figsize=(25,25))
sns.heatmap(df_corr, annot=True)
plt.show()

"""# Selecting feature and splitting Train and Test Data"""

X = df.drop(columns='status', axis=1)
y = df.status

X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X.shape

X_train.shape

y_train.shape

"""#Implementing min max scaler to the dataset"""

#Import min max scaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler((-1, 1))

scaler

df = scaler.fit_transform(df)

df

X_train.head(5)

"""# I forgot to scale the data, better late than never :D"""

# Fit and transform the entire DataFrame (except the target)
scaled_df = scaler.fit_transform(X)

# Convert the scaled data back to a DataFrame
scaled_df = pd.DataFrame(scaled_df, columns=X.columns)

# Concatenate the scaled features with the target variable
scaled_df_with_target = pd.concat([scaled_df, y.reset_index(drop=True)], axis=1)

# Displaying the scaled DataFrame with target variable
print("Scaled DataFrame with target:")
print(scaled_df_with_target)

df2 = scaled_df_with_target
df2

X = df2.drop(columns='status', axis=1)
y = df2.status

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.shape)
print(y_train.shape)

X_train.head()

"""#For this instance, I'm going to be using Logistic regression, K nearest neighbor, XGBoostClassifier, Suppport Vector Machine (SVM), LGBMClassifier, Logistic regression, and random forest

First lets import all of them
"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

import xgboost as xgb
from xgboost import XGBClassifier

import lightgbm as lgb
from lightgbm import LGBMClassifier

#Activating the algorithms

lr = LogisticRegression()
KNC = KNeighborsClassifier()
svm = SVC()
rf = RandomForestClassifier()
xgb_clf = XGBClassifier()
lgb_clf = LGBMClassifier()

"""# Hyperparameter tuning for our models, I will be using gridsearchCV

The evaluation will be based on accuracy.
"""

from sklearn.model_selection import GridSearchCV

models = {
    'LogisticRegression': LogisticRegression(),
    'KNeighborsClassifier': KNeighborsClassifier(),
    'SVM': SVC(),
    'RandomForestClassifier': RandomForestClassifier(),
    'XGBClassifier': XGBClassifier(),
}

param_grid = {
    'LogisticRegression': {'C': [0.1, 1, 10, 100], 'penalty': ['l2']},
    'KNeighborsClassifier': {'n_neighbors': [3, 5, 7, 9]},
    'SVM': {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},
    'RandomForestClassifier': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]},
    'XGBClassifier': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]},
}

#Storing result in a new array
results = {}
for model_name, model in models.items():
  grid_search = GridSearchCV(model, param_grid[model_name], cv=15, scoring='accuracy', error_score='raise')
  grid_search.fit(X, y)
  results[model_name] = {
      'best_params' : grid_search.best_params_,
      'best_score' : grid_search.best_score_
      }

for model_name, result in results.items():
    print(f"Results for {model_name}:")
    print("Best Parameters:", result['best_params'])
    print("Best CV Score:", result['best_score'])
    print()

"""# XGBClassifier has the best result

Let's put it to the test
"""

xgb_clf = XGBClassifier(
    learning_rate=0.1,
    max_depth=3,
    n_estimators=50,
    colsample_bytree=0.8,
    subsample=0.8,
    gamma = 1,
    reg_alpha = 0.1
)

unique_values_y = np.unique(y)
print(unique_values_y)

X_train.describe()

y_train.describe()

X_train_matched = X_train.iloc[:len(y_train)]

X_train = X_train_matched

xgb_clf.fit(X_train, y_train)

predict = xgb_clf.predict(X_test)
predict

pd.DataFrame({'Actual' : y_test, 'predict': predict},)

"""#Classification report"""

from sklearn.metrics import classification_report

report = classification_report(y_test, predict)

# Print the classification report
print(report)

"""# The result of the comparison between each model indicates that XGBClassifier is the best one for this particular dataset.

Now we have to save the model using pickle

"""

import pickle

with open('XGBClassifier_model.pkl', 'wb') as file:
  pickle.dump(xgb_clf, file)

